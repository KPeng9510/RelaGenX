# Domain-Agnostic Action Recognition: Pioneering Few-shot Adaptation in Video Analysis

Domain adaptation is essential for activity recognition, as common spatiotemporal architectures risk overfitting due to increased parameters arising from the temporal dimension. Unsupervised domain adaptation methods have been extensively studied, yet, they require large-scale unlabeled data from the target domain. In this work, we address \emph{Few-Shot Domain Adaptation for video-based Activity Recognition (FSDA-AR)}, which leverages a very small amount of labeled target videos to achieve effective adaptation. This setting is attractive and promising for applications, as it requires recording and labeling only a few, or even a single example per class in the target domain, which often includes activities that are rare yet crucial to recognize. We construct FSDA-AR benchmarks using five established datasets considering diverse domain types: UCF101, HMDB51, EPIC-KITCHEN, Sims4Action, and ToyotaSmartHome. Our results demonstrate that FSDA-AR performs comparably to unsupervised domain adaptation with significantly fewer (yet labeled) target domain samples. We further propose a novel approach, RelaGenX, to better leverage the few labeled target domain samples as knowledge guidance. RelaGenX encompasses a temporal relational attentive aggregation network that incorporates relation dropout, alongside a cross-domain information alignment mechanism. Furthermore, it integrates a mechanism for mixing features within a latent space, which is cognizant of data snippets. Our approach achieves state-of-the-art performance on all datasets within our FSDA-AR benchmark.

# Code release:

We are now preparing for the code releasement to make it tidy and readable. Once the preparing work is done, the code will be achievable in this github repo, thanks! 
